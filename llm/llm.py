import ollama
import re
import json

class F1AnalysisLLM:
    """
    Class to handle F1 analysis using a local language model (Ollama).
    This class is responsible for:
    1. Extracting parameters from user questions
    2. Generating analysis summaries based on data and graphs
    """
    def __init__(self, model="mistral"):
        """
        Initializes the language model.
        Args:
            model (str): Name of the model to use (default "mistral")
        """
        self.model = model
        
    def extract_analysis_params(self, user_message):
        """
        Extracts key parameters from the user's question using the LLM.
        
        Process:
        1. Builds a structured prompt requesting specific information
        2. Sends the prompt to the language model
        3. Attempts to parse the response as JSON
        4. Returns the extracted parameters or None if there's an error
        
        Args:
            user_message (str): User's question about F1
            
        Returns:
            dict: Dictionary with extracted parameters or None if there's an error
        """
        prompt = f"""
        Analiza la siguiente pregunta sobre Fórmula 1 y extrae:
        - piloto (nombre o apellido)
        - año/temporada
        - gran premio/circuito
        
        Pregunta: "{user_message}"
        
        Responde en formato JSON:
        {{
            "pilot": "nombre_piloto",
            "year": año,
            "track": "nombre_circuito"
        }}
        """
        
        # Sends the prompt to the model and gets the response
        response = ollama.chat(model=self.model, messages=[
            {'role': 'user', 'content': prompt}
        ])
        
        # Attempts to convert the response to JSON
        try:
            return json.loads(response['message']['content'])
        except:
            return None
    
    def generate_analysis_summary(self, analysis_data, graphs_info):
        """
        Generates a detailed analysis summary using the LLM.
        
        Process:
        1. Builds a prompt with analysis data and graph information
        2. Sends the prompt to the model requesting a structured analysis
        3. Returns the generated summary
        
        Args:
            analysis_data (dict): Analyzed event data
            graphs_info (dict): Information about generated graphs
            
        Returns:
            str: Analysis summary generated by the model
        """
        prompt = f"""
        Genera un análisis detallado del rendimiento del piloto en español basado en:
        
        Datos del evento: {analysis_data}
        Gráficos generados: {graphs_info}
        
        Incluye:
        - Resumen simple del rendimiento general del piloto
        - Puntos destacados del análisis
        - Conclusion general
        
        Mantén un tono profesional pero accesible a cualquier persona.
        """
        
        # Sends the prompt to the model and returns the generated analysis
        response = ollama.chat(model=self.model, messages=[
            {'role': 'user', 'content': prompt}
        ])
        
        return response['message']['content']